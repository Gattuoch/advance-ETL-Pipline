# End-to-End Advanced ETL Data Pipeline

## Project Overview

This project implements an **end-to-end advanced ETL (Extract, Transform, Load) data pipeline** designed to process large-scale datasets using distributed computing and modern analytics tools. The objective is to ingest data from multiple heterogeneous sources, transform it efficiently using **Apache PySpark**, store analytical outputs in **DuckDB**, and expose insights through an interactive **BI dashboard**.

The pipeline is fully orchestrated using **Apache Airflow**, ensuring reliability, scalability, and scheduled execution. This implementation satisfies all core and bonus requirements of the assignment.

---

## Business Problem

Organizations often collect data from multiple formats and sources (CSV, Parquet, APIs), making analytics difficult without a unified processing and storage layer. This project demonstrates how to:

* Integrate multiple raw data sources
* Perform scalable transformations on large datasets
* Store analytics-ready data in a high-performance analytical database
* Deliver actionable insights through BI dashboards

---

## Architecture

**Pipeline Flow:**

```
Data Sources (CSV / Parquet / API)
        â†“
Apache PySpark (Distributed Transformations)
        â†“
DuckDB (Analytical Data Warehouse)
        â†“
BI Tool (Tableau / Power BI)
```

**Orchestration:** Apache Airflow schedules and monitors each pipeline stage.

---

## Data Sources

This project integrates **three distinct data sources**, including at least one Parquet file:

1. **Climate CSV Data** â€“ Historical climate measurements
2. **Parquet Dataset** â€“ Structured columnar dataset for efficient reads
3. **Weather API (JSON)** â€“ Near real-time weather data ingestion

---

## Technologies Used

* **Apache PySpark** â€“ Distributed data transformation and aggregation
* **DuckDB** â€“ High-performance analytical database
* **Apache Airflow** â€“ Workflow orchestration and scheduling
* **Python** â€“ Data extraction, validation, and loading
* **Docker** â€“ Containerized development and execution
* **BI Tool (Tableau / Power BI)** â€“ Interactive dashboards and reporting

---

## ETL Pipeline Stages

### 1. Extract

* Read CSV files using Spark
* Load Parquet datasets using Spark
* Fetch JSON data from a weather API

### 2. Transform (PySpark)

* Data cleaning and schema normalization
* Time-based aggregations (yearly/monthly)
* Analytical transformations (averages, trends, rankings)

### 3. Load

* Persist transformed datasets into **DuckDB** tables
* Overwrite or append modes supported for analytical use cases

---

## Orchestration (Bonus Requirement)

The pipeline is orchestrated using **Apache Airflow**:

* DAGs manage extraction, transformation, and loading tasks
* Supports retries, logging, and scheduling
* Pipeline is scheduled to run automatically (e.g., daily or weekly)

---

## Project Structure

```
etl-datasets/
â”‚
â”œâ”€â”€ .astro/                         # Astro CLI configuration
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ .devcontainer/                  # Dev container setup
â”‚   â”œâ”€â”€ devcontainer.json
â”‚   â””â”€â”€ post_create_script.sh
â”‚
â”œâ”€â”€ dags/                           # Airflow DAGs (orchestration layer)
â”‚   â”œâ”€â”€ example_dag.py
â”‚   â”œâ”€â”€ extract_current_weather_data.py
â”‚   â”œâ”€â”€ extract_historical_weather_data.py
â”‚   â”œâ”€â”€ extract_parquet_data.py
â”‚   â”œâ”€â”€ transform_climate_data.py
â”‚   â”œâ”€â”€ transform_historical_weather.py
â”‚   â”œâ”€â”€ transform_climate_spark.py
â”‚   â””â”€â”€ start.py
â”‚
â”œâ”€â”€ include/                        # Shared utilities & configuration
â”‚   â”œâ”€â”€ global_variables/
â”‚   â”‚   â”œâ”€â”€ airflow_conf_variables.py
â”‚   â”‚   â”œâ”€â”€ constants.py
â”‚   â”‚   â””â”€â”€ user_input_variables.py
â”‚   â”œâ”€â”€ meteorology_utils.py
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ spark/                          # PySpark distributed transformations
â”‚   â”œâ”€â”€ transform_climate_spark.py
â”‚   â””â”€â”€ transform_historical_weather_spark.py
â”‚
â”œâ”€â”€ data/                           # Raw & intermediate datasets
â”‚   â”œâ”€â”€ climate_data/
â”‚   â”‚   â”œâ”€â”€ climate.csv
â”‚   â”‚   â””â”€â”€ historical_weather.parquet
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ transformed_data.parquet
â”‚
â”œâ”€â”€ dbt/                            # (Bonus) dbt models for DuckDB
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ bi/                             # BI dashboard outputs
â”‚   â”œâ”€â”€ BI_Dashboard_1.png
â”‚   â”œâ”€â”€ BI_Dashboard_2.png
â”‚   â””â”€â”€ BI_Dashboard_3.png
â”‚
â”œâ”€â”€ tests/                          # Unit & data quality tests
â”‚   â””â”€â”€ test_data_integrity.py
â”‚
â”œâ”€â”€ docker-compose.override.yml     # Local Airflow services override
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ profiles.yml                    # dbt DuckDB profile
â”œâ”€â”€ README.md                       # Project documentation
â””â”€â”€ .gitignore

```

---

## How to Run the Project

### Prerequisites

* Docker & Docker Compose
* Python 3.9+

### Steps

```bash
# Start Airflow and services
astro dev start

# Access Airflow UI
http://localhost:8080

# Trigger DAG manually or wait for schedule
```

DuckDB database is generated automatically during pipeline execution.

---
## BI Dashboard

DuckDB serves as the analytics backend for the BI layer.  
The transformed datasets are queried directly to generate insights such as trends,
aggregations, and performance metrics.

ðŸ“Š **Dashboard Screenshots**

### Dashboard 1
![Dashboard 1](bi/BI_Dashboard_1.png)

### Dashboard 2
![Dashboard 2](bi/BI_Dashboard_2.png)

### Dashboard 3
![Dashboard 3](bi/BI_Dashboard_3.png)

## Assignment Requirements Mapping

| Requirement             | Status      |
| ----------------------- | ----------- |
| â‰¥ 3 Data Sources        | âœ…           |
| Parquet Input           | âœ…           |
| PySpark Transformations | âœ…           |
| DuckDB Storage          | âœ…           |
| Orchestration Tool      | âœ… (Airflow) |
| BI Dashboard            | âœ…           |
| Scalable Architecture   | âœ…           |

---

## Team Members & Contributions

* **Gattuoch Chambang     1401298 1** â€“ Pipeline architecture & Airflow DAGs
* **Danial  Baye 1401105 2** â€“ PySpark transformations & data modeling
* **Abdihakiim   Mohamed 368713 3** â€“ DuckDB integration & BI dashboard

---

## Conclusion

This project demonstrates a production-style analytics pipeline using modern data engineering tools. It highlights best practices in scalable data processing, orchestration, and analytics delivery, fully aligned with the assignment requirements.

---

âœ… **Repository is public and submission-ready**
# advance-ETL-Pipline
