# End-to-End Advanced ETL Data Pipeline

## Project Overview

This project implements an **end-to-end advanced ETL (Extract, Transform, Load) data pipeline** designed to process large-scale datasets using distributed computing and modern analytics tools. The objective is to ingest data from multiple heterogeneous sources, transform it efficiently using **Apache PySpark**, store analytical outputs in **DuckDB**, and expose insights through an interactive **BI dashboard**.

The pipeline is fully orchestrated using **Apache Airflow**, ensuring reliability, scalability, and scheduled execution. This implementation satisfies all core and bonus requirements of the assignment.

---

## Business Problem

Organizations often collect data from multiple formats and sources (CSV, Parquet, APIs), making analytics difficult without a unified processing and storage layer. This project demonstrates how to:

* Integrate multiple raw data sources
* Perform scalable transformations on large datasets
* Store analytics-ready data in a high-performance analytical database
* Deliver actionable insights through BI dashboards

---

## Architecture

**Pipeline Flow:**

```
Data Sources (CSV / Parquet / API)
        â†“
Apache PySpark (Distributed Transformations)
        â†“
DuckDB (Analytical Data Warehouse)
        â†“
BI Tool (Tableau / Power BI)
```

**Orchestration:** Apache Airflow schedules and monitors each pipeline stage.

---

## Data Sources

This project integrates **three distinct data sources**, including at least one Parquet file:

1. **Climate CSV Data** â€“ Historical climate measurements
2. **Parquet Dataset** â€“ Structured columnar dataset for efficient reads
3. **Weather API (JSON)** â€“ Near real-time weather data ingestion

---

## Technologies Used

* **Apache PySpark** â€“ Distributed data transformation and aggregation
* **DuckDB** â€“ High-performance analytical database
* **Apache Airflow** â€“ Workflow orchestration and scheduling
* **Python** â€“ Data extraction, validation, and loading
* **Docker** â€“ Containerized development and execution
* **BI Tool (Tableau / Power BI)** â€“ Interactive dashboards and reporting

---

## ETL Pipeline Stages

### 1. Extract

* Read CSV files using Spark
* Load Parquet datasets using Spark
* Fetch JSON data from a weather API

### 2. Transform (PySpark)

* Data cleaning and schema normalization
* Time-based aggregations (yearly/monthly)
* Analytical transformations (averages, trends, rankings)

### 3. Load

* Persist transformed datasets into **DuckDB** tables
* Overwrite or append modes supported for analytical use cases

---

## Orchestration (Bonus Requirement)

The pipeline is orchestrated using **Apache Airflow**:

* DAGs manage extraction, transformation, and loading tasks
* Supports retries, logging, and scheduling
* Pipeline is scheduled to run automatically (e.g., daily or weekly)

---

## Project Structure

```
end-to-end-etl/
â”œâ”€â”€ dags/                  # Airflow DAG definitions
â”œâ”€â”€ include/               # Raw datasets (CSV / Parquet)
â”œâ”€â”€ spark/                 # PySpark transformation scripts
â”œâ”€â”€ plugins/               # Custom Airflow plugins (if any)
â”œâ”€â”€ docker-compose.yml     # Container orchestration
â”œâ”€â”€ Dockerfile             # Runtime environment
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # Project documentation
```

---

## How to Run the Project

### Prerequisites

* Docker & Docker Compose
* Python 3.9+

### Steps

```bash
# Start Airflow and services
astro dev start

# Access Airflow UI
http://localhost:8080

# Trigger DAG manually or wait for schedule
```

DuckDB database is generated automatically during pipeline execution.

---

## BI Dashboard

* DuckDB serves as the analytics backend
* BI tool connects directly to DuckDB
* Dashboards visualize trends, aggregations, and insights

ðŸ“Š Screenshots or setup instructions are included in the repository.

---

## Assignment Requirements Mapping

| Requirement             | Status      |
| ----------------------- | ----------- |
| â‰¥ 3 Data Sources        | âœ…           |
| Parquet Input           | âœ…           |
| PySpark Transformations | âœ…           |
| DuckDB Storage          | âœ…           |
| Orchestration Tool      | âœ… (Airflow) |
| BI Dashboard            | âœ…           |
| Scalable Architecture   | âœ…           |

---

## Team Members & Contributions

* **Member 1** â€“ Pipeline architecture & Airflow DAGs
* **Member 2** â€“ PySpark transformations & data modeling
* **Member 3** â€“ DuckDB integration & BI dashboard

---

## Conclusion

This project demonstrates a production-style analytics pipeline using modern data engineering tools. It highlights best practices in scalable data processing, orchestration, and analytics delivery, fully aligned with the assignment requirements.

---

âœ… **Repository is public and submission-ready**
# advance-ETL-Pipline
